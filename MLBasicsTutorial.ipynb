{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning (ML) Basics Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Basic Classification with Iris Dataset\n",
    "\n",
    "- **Objective:** Classify iris flowers (ดอกไอริส) into three species using sepal (กลีบเลี้ยง) and petal (กลีบดอก) measurements.\n",
    "- **Dataset:** The Iris dataset includes four features and a target for three species of Iris flowers:\n",
    "  - **Features (X):**\n",
    "    - Sepal Length (cm)\n",
    "    - Sepal Width (cm)\n",
    "    - Petal Length (cm)\n",
    "    - Petal Width (cm)\n",
    "  - **Target (y):**\n",
    "    - Species of Iris (0 for Setosa, 1 for Versicolour, 2 for Virginica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install pip install pandas numpy matplotlib seaborn scikit-learn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **sns.pairplot(...)**: It is used to visualize the relationships between pairs of dataset features, which reveals how they are related to each other.\n",
    "- **diag_kind=\"kde\"**: This optional parameter sets diagonal plots to kernel density estimates (kde), which shows data distribution as a smooth curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the data\n",
    "sns.pairplot(pd.DataFrame(X, columns=iris.feature_names), diag_kind=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **solver='saga':** The **'solver'** parameter used for optimization when fitting the logistic regression model, set to **'saga'** here, is effective for large datasets in Python's scikit-learn library.\n",
    "- **max_iter=2000:** This parameter sets the maximum number of iterations (or epochs) for the optimization algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model and train\n",
    "model = LogisticRegression(solver='saga', max_iter=2000)  # Using a different solver\n",
    "# Train the logistic regression model using labeled data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **confusion_matrix(...)**: This function creates a heatmap with Seaborn to compare actual and predicted values from a model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Confusion Matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Sentiment Analysis with IMDb Reviews\n",
    "\n",
    "- **Objective:** Determine whether movie reviews are positive or negative using text data.\n",
    "- **Dataset:** The IMDB dataset in TensorFlow's Keras includes:\n",
    "  - **Reviews (Text Data):**\n",
    "    - Every review is a sequence of word indices that correspond to the words used in the review text.\n",
    "    - The words are indexed by their overall frequency in the dataset. For example, the integer \"3\" represents the 3rd most frequent word.\n",
    "  - **Labels (Binary):**\n",
    "    - Every review is categorized as either 0, representing a negative sentiment, or 1, representing a positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **imdb.load_data(...):** Load the dataset, which stores training and test data with their respective labels, limited to 10,000 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Padding in natural language processing (NLP) and sequence-based tasks serves several purposes:** \n",
    "  - **Fixed Input Length:** The padding ensures that all input sequences are the same length, which makes it possible to process them in batches efficiently.\n",
    "  - **Batch Processing:** By standardizing sequence lengths in a batch, padding supports parallel processing and optimizes GPU use.\n",
    "  - **Model Compatibility:** It enables compatibility with neural networks, such as RNNs, LSTMs, and CNNs that need fixed-length inputs.\n",
    "  - **Prevents Data Loss:** Padding avoids truncating longer sequences, which might lose important information.\n",
    "  - **Facilitates Embeddings:** In deep learning, fixed-length sequences streamline the creation of consistent-dimensional embeddings for text tokens.\n",
    "\n",
    "\n",
    "- **tf.keras.preprocessing.sequence.pad_sequences:** To prepare the data, both the training and testing datasets are padded using a sequence length of 256, and any missing values are filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "train_data = tf.keras.preprocessing.sequence.pad_sequences(train_data, value=0, padding='post', maxlen=256)\n",
    "test_data = tf.keras.preprocessing.sequence.pad_sequences(test_data, value=0, padding='post', maxlen=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The model is constructed with:**\n",
    "  - An embedding layer (input vocabulary: 10,000, embedding size: 16).\n",
    "  - A global average pooling layer to reduce the sequence dimension.\n",
    "  - A 16-unit dense layer with ReLU activation.\n",
    "  - A single-unit dense layer with sigmoid activation for binary classification.\n",
    "\n",
    "- As for training, it is configured with the **'adam'** optimizer, using **'binary_crossentropy'** as the loss function, and tracking **'accuracy'** as a performance metric.\n",
    "- The **ReLU (Rectified Linear Unit)** activation function is used. It outputs the input value if it's positive, and zero if it's negative This introduces non-linearity to the model and helps in mitigating gradient vanishing issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential([\n",
    "  Embedding(10000, 16),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(16, activation='relu'),\n",
    "  Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_data, train_labels, epochs=30, batch_size=512, validation_data=(test_data, test_labels), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the training history\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "predictions = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **Receiver Operating Characteristic (ROC)** curve is created using predictions, which leads to two key metrics: False Positive Rate (FPR) and True Positive Rate (TPR). From these, the Area Under the Curve (AUC) is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve from predictions\n",
    "fpr, tpr, _ = roc_curve(test_labels, predictions)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Customer Segmentation with Mall Customer Data\n",
    "\n",
    "- **Objective:** Segment customers based on their spending patterns and characteristics.\n",
    "- **Dataset:** Mall Customer Segmentation Data (available on Kaggle), which contains the following fields:\n",
    "  - **CustomerID:** A unique identifier for each customer.\n",
    "  - **Gender:** The gender of the customer (e.g., Male, Female).\n",
    "  - **Age:** The age of the customer.\n",
    "  - **Annual Income (k$):** The customer's annual income in thousands of dollars.\n",
    "  - **Spending Score (1-100):** A score assigned to the customer based on their spending behavior and purchasing data. This score is on a scale from 1 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (replace with actual dataset path)\n",
    "df = pd.read_csv('Mall_Customers.csv')\n",
    "\n",
    "# One-hot encoding for the 'Gender' column\n",
    "df = pd.get_dummies(df, columns=['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects specific columns (income and spending score) for clustering.\n",
    "X = df.iloc[:, [1, 2]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "y_kmeans = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='rainbow')\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('Spending Score')\n",
    "plt.title('Customer Segments')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
