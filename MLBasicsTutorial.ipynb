{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning (ML) Basics Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In machine learning, especially for binary classification, select metrics based on your specific problem:\n",
    "- **Accuracy:** Good for balanced datasets.\n",
    "- **Precision:** When avoiding false positives is important.\n",
    "- **Recall:** Crucial when missing true positives is costly.\n",
    "- **F1-Score:** Balances precision and recall, useful for uneven class distributions.\n",
    "- **The area under the Receiver Operating Characteristic curve (AUC):** Evaluates performance across various thresholds, suitable for overall assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Basic Classification with Iris Dataset\n",
    "\n",
    "- **Objective:** Classify iris flowers (ดอกไอริส) into three species using sepal (กลีบเลี้ยง) and petal (กลีบดอก) measurements.\n",
    "- **Dataset:** The Iris dataset includes four features and a target for three species of Iris flowers:\n",
    "  - **Features (X):**\n",
    "    - Sepal Length (cm)\n",
    "    - Sepal Width (cm)\n",
    "    - Petal Length (cm)\n",
    "    - Petal Width (cm)\n",
    "  - **Target (y):**\n",
    "    - Species of Iris (0 for Setosa, 1 for Versicolour, 2 for Virginica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install pip install pandas numpy matplotlib seaborn scikit-learn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **sns.pairplot(...)**: It is used to visualize the relationships between pairs of dataset features, which reveals how they are related to each other.\n",
    "- **diag_kind=\"kde\"**: This optional parameter sets diagonal plots to kernel density estimates (kde), which shows data distribution as a smooth curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the data\n",
    "sns.pairplot(pd.DataFrame(X, columns=iris.feature_names), diag_kind=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **solver='saga':** The **'solver'** parameter used for optimization when fitting the logistic regression model, set to **'saga'** here, is effective for large datasets in Python's scikit-learn library.\n",
    "- **max_iter=2000:** This parameter sets the maximum number of iterations (or epochs) for the optimization algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model and train\n",
    "model = LogisticRegression(solver='saga', max_iter=2000)  # Using a different solver\n",
    "# Train the logistic regression model using labeled data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **confusion_matrix(...)**: This function creates a heatmap with Seaborn to compare actual and predicted values from a model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Confusion Matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Sentiment Analysis with IMDb Reviews\n",
    "\n",
    "- **Objective:** Determine whether movie reviews are positive or negative using text data.\n",
    "- **Dataset:** The IMDB dataset in TensorFlow's Keras includes:\n",
    "  - **Reviews (Text Data):**\n",
    "    - Every review is a sequence of word indices that correspond to the words used in the review text.\n",
    "    - The words are indexed by their overall frequency in the dataset. For example, the integer \"3\" represents the 3rd most frequent word.\n",
    "  - **Labels (Binary):**\n",
    "    - Every review is categorized as either 0, representing a negative sentiment, or 1, representing a positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **imdb.load_data(...):** Load the dataset, which stores training and test data with their respective labels, limited to 10,000 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of samples in the training dataset\n",
    "print(f'Training data sample count: {len(train_data)}')\n",
    "\n",
    "# Print the number of samples in the testing dataset\n",
    "print(f'Testing data sample count: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Padding in natural language processing (NLP) and sequence-based tasks serves several purposes:** \n",
    "  - **Fixed Input Length:** The padding ensures that all input sequences are the same length, which makes it possible to process them in batches efficiently.\n",
    "  - **Batch Processing:** By standardizing sequence lengths in a batch, padding supports parallel processing and optimizes GPU use.\n",
    "  - **Model Compatibility:** It enables compatibility with neural networks, such as RNNs, LSTMs, and CNNs that need fixed-length inputs.\n",
    "  - **Prevents Data Loss:** Padding avoids truncating longer sequences, which might lose important information.\n",
    "  - **Facilitates Embeddings:** In deep learning, fixed-length sequences streamline the creation of consistent-dimensional embeddings for text tokens.\n",
    "\n",
    "- **tf.keras.preprocessing.sequence.pad_sequences:** To prepare the data, both the training and testing datasets are padded using a sequence length of 256, and any missing values are filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "train_data = tf.keras.preprocessing.sequence.pad_sequences(train_data, value=0, padding='post', maxlen=256)\n",
    "test_data = tf.keras.preprocessing.sequence.pad_sequences(test_data, value=0, padding='post', maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of samples in the padded training dataset\n",
    "print(f'Padded training data sample count: {len(train_data)}')\n",
    "\n",
    "# Print the number of samples in the padded testing dataset\n",
    "print(f'Padded testing data sample count: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the word indices from the first sample in the training dataset\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The code below converts the word indices in the train[0] back to words using the word dictionary. In addition, We've limited our download to the top 10,000 most frequently used words, meaning some less common words in the movie reviews are not included. These words are represented as '&lt;UNK&gt;' in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()  # Retrieve a dictionary mapping words to their index numbers\n",
    "\n",
    "# Offset 3 from each index because the first three numbers (0, 1, and 2) are reserved for special markers: \n",
    "# \"padding (<PAD>),\" \"start of sequence (<START>),\" and \"unknown (<UNK>).\"\n",
    "index_word = {v + 3: k for k, v in word_index.items()}\n",
    "\n",
    "# The mapping of indices to words from the index_word dictionary\n",
    "# print(index_word)\n",
    "\n",
    "# Set the first index to 1 to indicate the beginning of a sequence and include reserved words in the dictionary\n",
    "index_word[0] = '<PAD>'\n",
    "index_word[1] = '<START>'\n",
    "index_word[2] = '<UNK>'\n",
    "\n",
    "# Create a dictionary to convert numeric indices back to words\n",
    "index = 0\n",
    "print(\" \".join([index_word.get(idx, '?') for idx in train_data[index]]))\n",
    "\n",
    "# Map output 'positive' if the training label at the given index is 1, otherwise output 'negative'\n",
    "print(\"positive\" if train_labels[index] == 1 else \"negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The model is constructed with:**\n",
    "  - The model uses 25,000 samples each for training and testing.\n",
    "  - It includes an embedding layer for a 10,000-word vocabulary with each word represented by a 16-dimensional vector.\n",
    "  - A global average pooling layer is applied to simplify the data by averaging over the sequence.\n",
    "  - There is a dense layer with 16 neurons using ReLU activation.\n",
    "  - The output layer is a single neuron with sigmoid activation, used for binary (positive/negative) classification.\n",
    "  - As for training, it is configured with the **'adam'** optimizer, using **'binary_crossentropy'** as the loss function, and tracking **'accuracy'** as a performance metric.\n",
    "  - The **ReLU (Rectified Linear Unit)** activation function is used. It outputs the input value if it's positive, and zero if it's negative This introduces non-linearity to the model and helps in mitigating gradient vanishing issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential([\n",
    "  Embedding(10000, 16),\n",
    "  GlobalAveragePooling1D(),      # It simply compresses the sequence of data into a single vector (per sample) by averaging\n",
    "  Dense(16, activation='relu'),  # 2 Hidden Layers\n",
    "  Dense(1, activation='sigmoid') # 1 Output Layer\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_data, train_labels, epochs=30, batch_size=512, validation_data=(test_data, test_labels), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the training history\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "predictions = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **Receiver Operating Characteristic (ROC)** curve is created using predictions and leads to two key metrics: False Positive Rate (FPR) on the x-axis and True Positive Rate (TPR) on the y-axis. From these, the Area Under the Curve (AUC) is calculated to measure the model's performance, indicating how well it can distinguish between positive and negative classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve from predictions\n",
    "fpr, tpr, _ = roc_curve(test_labels, predictions)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Customer Segmentation with Mall Customer Data\n",
    "\n",
    "- **Objective:** Segment customers based on their spending patterns and characteristics.\n",
    "- **Dataset:** Mall Customer Segmentation Data (available on Kaggle), which contains the following fields:\n",
    "  - **CustomerID:** A unique identifier for each customer.\n",
    "  - **Gender:** The gender of the customer (e.g., Male, Female).\n",
    "  - **Age:** The age of the customer.\n",
    "  - **Annual Income (k$):** The customer's annual income in thousands of dollars.\n",
    "  - **Spending Score (1-100):** A score assigned to the customer based on their spending behavior and purchasing data. This score is on a scale from 1 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (replace with actual dataset path)\n",
    "df = pd.read_csv('Mall_Customers.csv')\n",
    "\n",
    "# One-hot encoding for the 'Gender' column\n",
    "df = pd.get_dummies(df, columns=['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects specific columns (annual income and spending score) for clustering.\n",
    "X = df.iloc[:, [2, 3]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "y_kmeans = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='rainbow')\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('Spending Score')\n",
    "plt.title('Customer Segments')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
