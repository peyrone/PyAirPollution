{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning (ML) and Deep Learning (DL) Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1: Predictive Analysis with the Titanic Dataset\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- **Objective:** Predict whether a passenger survived the Titanic disaster using logistic regression.\n",
    "- **Dataset:** The Titanic dataset is a classic dataset available on Kaggle. It includes passenger information from the Titanic disaster and can be used to predict survival outcomes. The Titanic dataset fields:\n",
    "  - **PassengerId:** A unique number for each passenger.\n",
    "  - **Pclass:** The ticket class (1st, 2nd, or 3rd class).\n",
    "  - **Name:** The passenger's name.\n",
    "  - **Sex:** The passenger's gender.\n",
    "  - **Age:** How old the passenger is.\n",
    "  - **SibSp:** Number of siblings or spouses on board.\n",
    "  - **Parch:** Number of parents or children on board.\n",
    "  - **Ticket:** The passenger's ticket number.\n",
    "  - **Fare:** How much the ticket cost.\n",
    "  - **Cabin:** The cabin number where the passenger stayed.\n",
    "  - **Embarked:** Where the passenger got on the ship (C = Cherbourg, Q = Queenstown, S = Southampton).\n",
    "- **Tasks:**\n",
    "  - Load the dataset and perform exploratory data analysis.\n",
    "  - **Preprocess the data: handle missing values, convert categorical data to numerical, etc.**\n",
    "  - Split the data into training and testing sets.\n",
    "  - Build a logistic regression model to predict survival.\n",
    "  - Evaluate the model's performance using accuracy, precision, and recall metrics.\n",
    "- **Hints:**\n",
    "  - Pay attention to columns, such as **'Age'**, **'Sex'**, **'Pclass'**, and **'Fare'**.\n",
    "  - Use libraries, such as Pandas for data manipulation, Scikit-learn for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install pip install pandas numpy matplotlib seaborn scikit-learn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv('Titanic/titanic_train.csv')\n",
    "test_data = pd.read_csv('Titanic/titanic_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "# TODO: Handle missing values, convert categorical data to numerical\n",
    "# Example: data['Age'].fillna(data['Age'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select features and target variable\n",
    "# Example: X = data[['Pclass', 'Age']]\n",
    "# Example: y = data['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain the Titanic dataset preprocessing steps:**\n",
    "\n",
    "- **Define Feature Sets:**\n",
    "  - **numeric_transformer:** Lists number-based data, e.g., **'Age'**, **'Fare'**, etc.\n",
    "  - **category_transformer:** Lists category-based data, e.g., **'Pclass'**, **'Sex'**, etc.\n",
    "- **Numeric Data Setup:** It creates a pipeline for transforming numeric features with two steps.\n",
    "  - **Imputer:** Fixes missing number data by using the median.\n",
    "  - **Scaler:** Makes sure all number data is on the same scale.\n",
    "- **Category Data Setup:** It creates a pipeline for transforming categorical features with two steps.\n",
    "  - **Imputer:** Fixes missing category data by labeling it **'missing'**.\n",
    "  - **OneHot:** Changes category data to a numeric format the computer can use.\n",
    "- **Combine Steps:**\n",
    "  - Uses a **'ColumnTransformer'** to apply the correct fixes to each type of data.\n",
    "  - Gets everything ready for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "  ('imputer', SimpleImputer(strategy='median')),\n",
    "  ('scaler', StandardScaler())])\n",
    "\n",
    "category_transformer = Pipeline(steps=[\n",
    "  ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "  ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "  transformers=[\n",
    "    ('num', numeric_transformer, numeric_transformer),\n",
    "    ('cat', category_transformer, category_transformer)])\n",
    "\n",
    "X = train_data.drop('Survived', axis=1)\n",
    "y = train_data['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the test data (use 'transform' not 'fit_transform')\n",
    "X_test = test_data.drop('PassengerId', axis=1)  # Assuming 'PassengerId' is the only non-feature column\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Predict on the test data\n",
    "test_predictions = model.predict(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate the model\n",
    "predictions = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2: Time Series Forecasting with the Air Quality Dataset using LSTM\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- **Objective:** Forecast future air pollution levels (e.g., NO2 concentration) using time series analysis.\n",
    "- **Dataset:** The Air Quality Time Series dataset from the UCI Machine Learning Repository provides multi-year air quality data for time series analysis and forecasting. The Air Quality dataset fields:\n",
    "  - **Date:** When the data was recorded.\n",
    "  - **Time:** Time of day for the data.\n",
    "  - **CO(GT):** Carbon Monoxide level.\n",
    "  - **PT08.S1(CO):** Sensor response for CO.\n",
    "  - **NMHC(GT):** Non-Methane Hydrocarbons level.\n",
    "  - **C6H6(GT):** Benzene level.\n",
    "  - **PT08.S2(NMHC):** Sensor response for NMHC.\n",
    "  - **NOx(GT):** Nitrogen Oxides level.\n",
    "  - **PT08.S3(NOx):** Sensor response for NOx.\n",
    "  - **NO2(GT):** Nitrogen Dioxide level.\n",
    "  - **PT08.S4(NO2):** Sensor response for NO2.\n",
    "  - **PT08.S5(O3):** Sensor response for Ozone.\n",
    "  - **T:** Temperature.\n",
    "  - **RH:** Relative Humidity.\n",
    "  - **AH:** Absolute Humidity.\n",
    "- **Tasks:**\n",
    "  - Load the dataset and perform initial exploratory data analysis focused on time series aspects.\n",
    "  - **Handle missing values and preprocess the data for time series analysis.**\n",
    "  - Visualize the time series data to understand trends, seasonality, and noise.\n",
    "  - Use a time series forecasting method, such as LSTM to predict future pollution levels.\n",
    "  - Evaluate the model's forecasting accuracy.\n",
    "- **Hints:**\n",
    "  - Investigate how NO2 levels change over time.\n",
    "  - Consider resampling the data (e.g., daily averages) if working with high granularity data.\n",
    "  - Utilize libraries like Pandas for data manipulation, statsmodels for TensorFlow/Keras for LSTM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('AirQualityUCI.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "# TODO: Handle missing values and set the datetime index\n",
    "# Example: data['Date'] = pd.to_datetime(data['Date'])\n",
    "# data.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the NO2 column\n",
    "no2_values = data['NO2(GT)'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_no2 = scaler.fit_transform(no2_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_size = int(len(scaled_no2) * 0.67)\n",
    "test_size = len(scaled_no2) - train_size\n",
    "train, test = scaled_no2[0:train_size,:], scaled_no2[train_size:len(scaled_no2),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "  X, Y = [], []\n",
    "  for i in range(len(dataset)-look_back-1):\n",
    "    a = dataset[i:(i+look_back), 0]\n",
    "    X.append(a)\n",
    "    Y.append(dataset[i + look_back, 0])\n",
    "  return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape into X=t and Y=t+1\n",
    "look_back = 1\n",
    "X_train, Y_train = create_dataset(train, look_back)\n",
    "X_test, Y_test = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train, Y_train, epochs=100, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert predictions\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "Y_train = scaler.inverse_transform([Y_train])\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "Y_test = scaler.inverse_transform([Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot baseline and predictions\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(scaler.inverse_transform(scaled_no2))\n",
    "plt.plot(np.concatenate((train_predict, test_predict)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
