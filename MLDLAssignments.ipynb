{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning (ML) and Deep Learning (DL) Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1: Predictive Analysis with the Titanic Dataset\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- **Objective:** Predict whether a passenger survived the Titanic disaster using logistic regression.\n",
    "- **Dataset:** The Titanic dataset is a classic dataset available on Kaggle. It includes passenger information from the Titanic disaster and can be used to predict survival outcomes. The Titanic dataset fields:\n",
    "  - **PassengerId:** A unique number for each passenger.\n",
    "  - **Pclass:** The ticket class (1st, 2nd, or 3rd class).\n",
    "  - **Name:** The passenger's name.\n",
    "  - **Sex:** The passenger's gender.\n",
    "  - **Age:** How old the passenger is.\n",
    "  - **SibSp:** Number of siblings or spouses on board.\n",
    "  - **Parch:** Number of parents or children on board.\n",
    "  - **Ticket:** The passenger's ticket number.\n",
    "  - **Fare:** How much the ticket cost.\n",
    "  - **Cabin:** The cabin number where the passenger stayed.\n",
    "  - **Embarked:** Where the passenger got on the ship (C = Cherbourg, Q = Queenstown, S = Southampton).\n",
    "- **Tasks:**\n",
    "  - Load the dataset and perform exploratory data analysis.\n",
    "  - Split the data into training and testing sets.\n",
    "  - **Preprocess the data: handle missing values, convert categorical data to numerical, etc.**\n",
    "  - Build a logistic regression model to predict survival.\n",
    "  - Create a submission file containing the **'PassengerId'** and **'Survived'** columns with the test predictions and save it to a CSV file.\n",
    "  - Evaluate the model's performance using accuracy, precision, and recall metrics.\n",
    "- **Hints:**\n",
    "  - Pay attention to columns, such as **'Age'**, **'Sex'**, **'Pclass'**, and **'Fare'**.\n",
    "  - Use libraries, such as Pandas for data manipulation, Scikit-learn for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install pip install pandas numpy matplotlib seaborn scikit-learn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv('TitanicData/titanic_train.csv')\n",
    "test_data = pd.read_csv('TitanicData/titanic_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numeric and categorical features\n",
    "numeric_features = ['Age', 'Fare']\n",
    "categorical_features = ['Sex', 'Embarked', 'Pclass']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain the Titanic dataset preprocessing steps:**\n",
    "\n",
    "- **Define Feature Sets:**\n",
    "  - **numeric_transformer:** Lists number-based data, e.g., **'Age'**, **'Fare'**, etc.\n",
    "  - **category_transformer:** Lists category-based data, e.g., **'Pclass'**, **'Sex'**, etc.\n",
    "- **Numeric Data Setup:** It creates a pipeline for transforming numeric features with two steps.\n",
    "  - **Imputer:** Fixes missing number data by using the median.\n",
    "  - **Scaler:** Makes sure all number data is on the same scale.\n",
    "- **Category Data Setup:** It creates a pipeline for transforming categorical features with two steps.\n",
    "  - **Imputer:** Fixes missing category data by labeling it **'missing'**.\n",
    "  - **OneHot:** Changes category data to a numeric format the computer can use.\n",
    "- **Combine Steps:**\n",
    "  - Uses a **'ColumnTransformer'** to apply the correct fixes to each type of data.\n",
    "  - Gets everything ready for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "  ('imputer', SimpleImputer(strategy='median')),\n",
    "  ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "  ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "  ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "  transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "X = train_data.drop('Survived', axis=1)\n",
    "y = train_data['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the preprocessor to X_train and transform both X_train and X_val\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_val_transformed = preprocessor.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the validation set\n",
    "val_predictions = model.predict(X_val_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the validation set\n",
    "accuracy = accuracy_score(y_val, val_predictions)\n",
    "precision = precision_score(y_val, val_predictions)\n",
    "recall = recall_score(y_val, val_predictions)\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(f'Validation Precision: {precision}')\n",
    "print(f'Validation Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the test data\n",
    "X_test = test_data.copy()\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "test_predictions = model.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions to a CSV file\n",
    "submission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': test_predictions})\n",
    "submission.to_csv('TitanicData/titanic_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot the precision-recall curve\n",
    "probs = model.predict_proba(X_val_transformed)[:, 1]\n",
    "precision, recall, _ = precision_recall_curve(y_val, probs)\n",
    "auc_score = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title(f'Precision-Recall Curve (AUC={auc_score:.2f})')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **An AUC of 0.87 indicates the model is effective at identifying the correct class, with values closer to 1 indicating better performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2: Time Series Forecasting with the Air Quality Dataset using LSTM\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- **Objective:** Forecast future air pollution levels (e.g., NO2 concentration) using time series analysis.\n",
    "- **Dataset:** The Air Quality Time Series dataset from the UCI Machine Learning Repository provides multi-year air quality data for time series analysis and forecasting. The Air Quality dataset fields:\n",
    "  - **Date:** When the data was recorded.\n",
    "  - **Time:** Time of day for the data.\n",
    "  - **CO(GT):** Carbon Monoxide level.\n",
    "  - **PT08.S1(CO):** Sensor response for CO.\n",
    "  - **NMHC(GT):** Non-Methane Hydrocarbons level.\n",
    "  - **C6H6(GT):** Benzene level.\n",
    "  - **PT08.S2(NMHC):** Sensor response for NMHC.\n",
    "  - **NOx(GT):** Nitrogen Oxides level.\n",
    "  - **PT08.S3(NOx):** Sensor response for NOx.\n",
    "  - **NO2(GT):** Nitrogen Dioxide level.\n",
    "  - **PT08.S4(NO2):** Sensor response for NO2.\n",
    "  - **PT08.S5(O3):** Sensor response for Ozone.\n",
    "  - **T:** Temperature.\n",
    "  - **RH:** Relative Humidity.\n",
    "  - **AH:** Absolute Humidity.\n",
    "- **Tasks:**\n",
    "  - Load the dataset and perform initial exploratory data analysis focused on time series aspects.\n",
    "  - **Handle missing values and preprocess the data for time series analysis.**\n",
    "  - Visualize the time series data to understand trends, seasonality, and noise.\n",
    "  - Use a time series forecasting method, such as LSTM to predict future pollution levels.\n",
    "  - Evaluate the model's forecasting accuracy.\n",
    "- **Hints:**\n",
    "  - Investigate how NO2 levels change over time.\n",
    "  - Consider resampling the data (e.g., daily averages) if working with high granularity data.\n",
    "  - Utilize libraries such as Pandas for data manipulation, TensorFlow and Keras for building LSTM models, and statsmodels for statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('AirQualityUCI.csv', delimiter=';', parse_dates=['Date'], dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"Unnamed\" columns\n",
    "data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# Remove rows where \"NO2(GT)\" column is less than 0\n",
    "data = data[data['NO2(GT)'] >= 0]\n",
    "\n",
    "# Handle missing values and set the datetime index\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the NO2 column\n",
    "no2_values = data['NO2(GT)'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_no2 = scaler.fit_transform(no2_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_size = int(len(scaled_no2) * 0.6)\n",
    "test_size = len(scaled_no2) - train_size\n",
    "train, test = scaled_no2[0:train_size,:], scaled_no2[train_size:len(scaled_no2),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "  X, Y = [], []\n",
    "  for i in range(len(dataset)-look_back-1):\n",
    "    a = dataset[i:(i+look_back), 0]\n",
    "    X.append(a)\n",
    "    Y.append(dataset[i + look_back, 0])\n",
    "  return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape into X=t and Y=t+7\n",
    "look_back = 7\n",
    "X_train, Y_train = create_dataset(train, look_back)\n",
    "X_test, Y_test = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = X_train.reshape(X_train.shape[0], look_back, -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], look_back, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the LSTM network with Dropout\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(look_back, 1), return_sequences=True, \n",
    "               kernel_regularizer=L1L2(l1=1e-5, l2=1e-4)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(25, return_sequences=False, \n",
    "               kernel_regularizer=L1L2(l1=1e-5, l2=1e-4)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, Y_train, epochs=100, batch_size=32, verbose=2, callbacks=[early_stopping], validation_split=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **scaler.inverse_transform:** It is used to convert the model's predictions and target values back to their original units after they were scaled during preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert predictions\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "Y_train = scaler.inverse_transform([Y_train])\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "Y_test = scaler.inverse_transform([Y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The meaning of the following evaluation metrics:**\n",
    "- **RMSE (Root Mean Squared Error):** It is a standard way to measure the error of a model in predicting quantitative data.\n",
    "- **MAE (Mean Absolute Error):** It measures the average magnitude of the errors in a set of predictions, without considering their direction.\n",
    "\n",
    "**Comparing training and test metrics for model evaluation:**\n",
    "- **Training RMSE and MAE:** Indicate how well the model fits the training data.\n",
    "- **Test RMSE and MAE:** Show how well the model is expected to perform on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(Y_train[0], train_predict[:,0]))\n",
    "train_mae = mean_absolute_error(Y_train[0], train_predict[:,0])\n",
    "test_rmse = np.sqrt(mean_squared_error(Y_test[0], test_predict[:,0]))\n",
    "test_mae = mean_absolute_error(Y_test[0], test_predict[:,0])\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Training RMSE: {train_rmse:.2f}')\n",
    "print(f'Training MAE: {train_mae:.2f}')\n",
    "print(f'Test RMSE: {test_rmse:.2f}')\n",
    "print(f'Test MAE: {test_mae:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot baseline and predictions\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(scaler.inverse_transform(scaled_no2), label='Actual')\n",
    "plt.plot(np.concatenate((train_predict, test_predict)), label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('NO2 Levels')\n",
    "plt.title('Comparison of Actual and Predicted NO2 Levels')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
